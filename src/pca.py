# -*- coding: utf-8 -*-
"""PCA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hIfhdEWByVCYfn46AQZ91SIj5Yuij5bI
"""

import pandas as pd
import networkx as nx

grouped_df = pd.read_csv('/content/credit_card.csv')

columns_to_drop = ['TX_TIME_SECONDS', 'TX_FRAUD_SCENARIO', 'TX_DURING_WEEKEND', 'CUSTOMER_ID_NB_TX_1DAY_WINDOW', 'CUSTOMER_ID_AVG_AMOUNT_1DAY_WINDOW']

from google.colab import drive
drive.mount('/content/drive')

grouped_df_dropped = grouped_df.drop(columns=columns_to_drop)

grouped_df_dropped.to_csv('/content/credit_card_2.csv', index=False)

import pandas as pd

# CSV 파일 읽기
file_path = '/content/credit_card.csv'
df = pd.read_csv(file_path)

# TX_FRAUD 값에 따라 데이터를 나누기
fraud_df = df[df['TX_FRAUD'] >= 1]
non_fraud_df = df[df['TX_FRAUD'] < 1]

# 동일한 CUSTOMER_ID를 가진 데이터 합치기
fraud_grouped = fraud_df.groupby('CUSTOMER_ID').sum()
non_fraud_grouped = non_fraud_df.groupby('CUSTOMER_ID').sum()

# 결과를 파일로 저장
fraud_grouped.to_csv('fraud_transactions.csv', index=False)
non_fraud_grouped.to_csv('non_fraud_transactions.csv', index=False)

#변수들 간 상관관계 파악
import pandas as pd

# CSV 파일 읽기
fraud_transactions = pd.read_csv('fraud_transactions.csv')

# 'TX_DATETIME' 열 제거
fraud_transactions = fraud_transactions.drop(columns=['TX_DATETIME'])

import seaborn as sns
import matplotlib.pyplot as plt

# 상관 행렬 계산
corr_matrix = fraud_transactions.corr()

# 상관 행렬 시각화
plt.figure(figsize=(12, 10))
sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap='coolwarm')
plt.title('Correlation Matrix of Fraud Transactions')
plt.show()

#상관계수를 이용한 계층 구분 방법
#1. 변수 표준화

from sklearn.preprocessing import StandardScaler
#변수들 간 상관관계 파악
import pandas as pd

# CSV 파일 읽기
non_fraud_transactions = pd.read_csv('non_fraud_transactions.csv')

# 'TX_DATETIME' 열 제거
non_fraud_transactions = non_fraud_transactions.drop(columns=['TX_DATETIME'])

# 표준화
scaler = StandardScaler()
scaled_data = scaler.fit_transform(non_fraud_transactions)

#2. 주성분 분석
from sklearn.decomposition import PCA

# 주성분 분석
pca = PCA(n_components=2)  # 주요 성분 2개 추출 (시각화를 위해 2차원으로 축소)
pca_result = pca.fit_transform(scaled_data)

#3. 클러스터링
from sklearn.cluster import KMeans

# K-means 클러스터링
kmeans = KMeans(n_clusters=10, random_state=0)  # 10개의 클러스터로 나눔
clusters = kmeans.fit_predict(pca_result)

# 클러스터링 결과를 원래 데이터프레임에 추가
non_fraud_transactions['Cluster'] = clusters
non_fraud_transactions.to_csv(f'/content/non_fraud_transactions_Clustered.csv', index=False)

import matplotlib.pyplot as plt

plt.figure(figsize=(10, 8))
plt.scatter(pca_result[:, 0], pca_result[:, 1], c=clusters, cmap='viridis')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('PCA of Fraud Transactions')
plt.colorbar()
plt.show()

import os
# 각 클러스터를 별도의 파일로 저장
output_dir = 'clusters'
if not os.path.exists(output_dir):
    os.makedirs(output_dir)

for i in range(10):
    cluster_data = non_fraud_transactions[non_fraud_transactions['Cluster'] == i]
    cluster_data.to_csv(f'{output_dir}/cluster_{i}.csv', index=False)
    print(f"Cluster {i}:\n", cluster_data.head())

